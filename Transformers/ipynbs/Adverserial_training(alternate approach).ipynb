{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "functional_experimental_generator_pretraining.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbpedia/RDF2text-GAN/blob/master/Transformers/Adverserial_training(alternate%20approach).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "### GAN Class for adverserial training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpEPUovEETH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! pip install tf-nightly-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JjJJyJTZYebt",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import unicodedata\n",
        "import re\n",
        "from re import finditer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fd1NWMxjfsDd"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XR8J2UYJYZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "file_path = \"/content/gdrive/My Drive/f_data.txt\"\n",
        "test_path = \"/content/gdrive/My Drive/data/processed_graphs/eng/gat/test_data.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSKfG9NNogqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pretraining import *\n",
        "from transformer_generator import *\n",
        "from transformer_discriminator import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkXk7ZMDZ-w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_dataset, tokenizer_txt = create_generator_dataset(file_path, BATCH_SIZE=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbX9_UgBb_jP",
        "colab_type": "text"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLjMhL73LKBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    real_loss = loss_object(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = loss_object(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "#Primary loss for plain adverserial training\n",
        "def generator_loss(real_output, fake_output):\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    loss_ = loss_object(tf.ones_like(fake_output), fake_output)\n",
        "    return  loss_ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zVjWCxFNcgbt"
      },
      "source": [
        "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
        "\n",
        "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
        "\n",
        "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lnJn5SLA2ahP",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "target_vocab_size = tokenizer_txt.vocab_size + 2\n",
        "input_vocab_size = target_vocab_size\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7r4scdulztRx",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxEs9TvYov9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrTP--5o7UyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_MAX_LEN = 250\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "discriminator = TransformerDiscriminator(tokenizer_txt.vocab_size+2, maxlen=DATA_MAX_LEN)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C88xrK1FNuLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generator.load_weights('./generator_weights.h5')\n",
        "#discriminator.load_weights('./discriminator_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fzuf06YZp66w"
      },
      "source": [
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-4_VP7bYJXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAN(keras.Model):\n",
        "\n",
        "    def __init__(self, discriminator, generator, tokenizer, batch_size=16):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.tokenizer_txt = tokenizer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, d_loss, g_loss):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.g_loss = g_loss\n",
        "        self.d_loss = d_loss\n",
        "\n",
        "    def pad(self, tensor, maxlen=250):\n",
        "      return tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                            padding='post',\n",
        "                                                            value=0,\n",
        "                                                            maxlen=maxlen)\n",
        "    @tf.function  \n",
        "    def tf_disc_predict(self, input_):\n",
        "\n",
        "      def get_disc_predictions(input_):\n",
        "        return self.discriminator(input_)\n",
        "\n",
        "      rv = tf.py_function(get_disc_predictions, inp=[input_], Tout=[tf.float32])\n",
        "      return rv\n",
        "\n",
        "    @tf.function( experimental_relax_shapes=True )\n",
        "    def tf_gen_batch(self, preds, inp,  tar):\n",
        "\n",
        "      def gen_batch(preds, inp, tar, max_len = 100):\n",
        "\n",
        "        def decode_text(array, tokenizer):\n",
        "          return tokenizer.decode([i for i in array if 0<i< tokenizer.vocab_size])\n",
        "\n",
        "        disc_data = []\n",
        "        for sent in preds:\n",
        "          unparsed = decode_text(sent, self.tokenizer_txt)\n",
        "          retokenized = self.tokenizer_txt.encode(unparsed.split('<end>')[0]+'<end>')\n",
        "          disc_data.append(padded)\n",
        "        \n",
        "        disc_data = self.pad(disc_data)\n",
        "        gens = self.pad(tf.concat([inp, disc_data], axis=-1, name='concat'))\n",
        "        real = self.pad(tf.concat([inp, tar], axis=-1, name='concat'))\n",
        "        all_data = tf.concat([gens, real], axis=0)\n",
        "        all_labels = tf.concat([ tf.zeros((self.batch_size, 1)) ,\n",
        "                                 tf.ones((self.batch_size, 1))],\n",
        "                                axis=0)\n",
        "        \n",
        "        return all_data, all_labels, gens\n",
        "\n",
        "      all_data, all_labels, gens = tf.py_function(gen_batch, inp=[preds, inp, tar], Tout=[tf.int32, tf.float32, tf.float32])\n",
        "\n",
        "      return all_data, all_labels, gens\n",
        "\n",
        "    def train_step(self, data):\n",
        "        inp, tar = data\n",
        "        tar_inp = tar[:, :-1]\n",
        "        tar_real = tar[:, 1:]\n",
        "        \n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "        predictions, _ = generator(inp, tar_inp, \n",
        "                                 False, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    \n",
        "        batch_pred = tf.argmax(predictions, axis=-1)\n",
        "        all_, labels, gens = self.tf_gen_batch(batch_pred, inp, tar)\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(all_)\n",
        "            d_loss_ = self.d_loss(labels, predictions)\n",
        "\n",
        "        grads = tape.gradient(d_loss_, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "        \n",
        "        # Train the generator \n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(gens)\n",
        "            predictions = self.tf_disc_predict(gens)\n",
        "\n",
        "            # Assemble labels that say \"all real images\"\n",
        "            misleading_labels = tf.ones((self.batch_size, 1))\n",
        "            g_loss = self.g_loss(misleading_labels, predictions)\n",
        "\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LKpoA6q1sJFj",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHfBV53AF_V6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan = GAN(discriminator=discriminator, generator=generator, tokenizer=tokenizer_txt)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    d_loss = discriminator_loss,\n",
        "    g_loss= generator_loss\n",
        ")\n",
        "\n",
        "# To limit execution time, we only train on 100 batches. You can train on\n",
        "# the entire dataset. You will need about 20 epochs to get nice results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5MRTIDPG6VN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan.fit(train_dataset, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZRz8DCxOCY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def render_preds(batch_pred, inp, tar, n=2):\n",
        "    print(type(batch_pred), type(inp), batch_pred.shape, inp.shape)\n",
        "    for (ind,i) in enumerate(batch_pred):\n",
        "      print('\\n| Predicted: ', decode_text(i, tokenizer_txt))\n",
        "      print('| True: ', decode_text(tar[ind], tokenizer_txt))\n",
        "      print('| Input RDF: ', decode_text(inp[ind], tokenizer_txt))\n",
        "      print()\n",
        "      if ind==n:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QfcsSWswSdGV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-3kov6Dc6LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_(inp_sentence):\n",
        "\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "  decoder_input = [tokenizer_txt.vocab_size]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer_txt.vocab_size+1:\n",
        "      return tf.squeeze(output, axis=0)\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FahpXe9at5nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH=250\n",
        "rdfb, txtb = next(iter(train_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZGs83toe6HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sentence = evaluate_(rdfb[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbPUmxKcUHgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decode_text(predicted_sentence, tokenizer_txt)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
