{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "discriminator_pretraining (multi-input model)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbpedia/RDF2text-GAN/blob/master/discriminator_pretraining_(multi_input_model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S09bm93WbSyt"
      },
      "source": [
        "## Pre-train Discriminator before adverserial training\n",
        "\n",
        "### Update :\n",
        "The discriminator is trained using two input layers, one for the rdf sequence, and one for the corresponding text sequence, per training instance. The idea here is that the discriminator sees two sequences and learns to predict whether or not they are related to each other, in terms of content conveyed. \n",
        "\n",
        "\n",
        "- Adapted from : Original Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cmSKLq9HbSyu",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIr5JiLoblx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2b2a708-3acf-4065-91bd-2695449b192e"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "file_path = \"/content/gdrive/My Drive/f_data.txt\"\n",
        "test_path = \"/content/gdrive/My Drive/data/processed_graphs/eng/gat/test_data.txt\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twNqfmM_wkps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pretraining\n",
        "from pretraining import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bc0XjkNwM2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dfa5a7b4-b765-4744-95a5-7705bb666395"
      },
      "source": [
        "train_dataset, tokenizer_txt = create_generator_dataset(file_path)\n",
        "Xr_train, Xr_test, Xt_train, Xt_test, y_train, y_test = create_discriminator_dataset_2(train_dataset)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max source length :  133 \n",
            "Max target length :  87\n",
            "Max RDF sequence size:  135\n",
            "Max text sequence size:  135\n",
            "dataset shape:  (68704, 135) (68704, 135) (68704, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-R5rc6rYtuQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "84bbe207-f801-4eb2-a940-1ea2d4577594"
      },
      "source": [
        "def probe(n, split='train'):\n",
        "    if split=='test':\n",
        "        print(decode_text(Xr_train[n], tokenizer_txt))\n",
        "        print(decode_text(Xt_train[n], tokenizer_txt))\n",
        "        print(y_train[n])\n",
        "    else:\n",
        "        print(decode_text(Xr_test[n], tokenizer_txt))\n",
        "        print(decode_text(Xt_test[n], tokenizer_txt))\n",
        "        print(y_test[n])\n",
        "\n",
        "probe(251, 'test')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> <triple> acharya institute of technology affiliation visvesvaraya technological university </triple> <triple> acharya institute of technology was given the 'technical campus' status by all india council for technical education </triple> <triple> all india council for technical education location mumbai </triple> <triple> karnataka has to its northeast telangana </triple> <triple> visvesvaraya technological university city belgaum </triple> <triple> karnataka has to its west arabian sea </triple> <triple> acharya institute of technology state karnataka </triple> <end>\n",
            "<start> the acharya institute of technology in karnataka state is affiliated with visvesvaraya technological university in belgium . it was given its technical campus' status by all india council for technical education in mumbai . karnataka has the arabian sea to its west and telagana to its northeast . <end>\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzq0rpKGg3KU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ebc22458-6fe6-4145-d1a1-0740485a1fcb"
      },
      "source": [
        "probe(251)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> <triple> aip advances editor vincent h . crespi , bernard s . gerstman , a . t . charlie johnson , masaaki tanaka , enge g . wang </triple> <end>\n",
            "<start> aleksandre guruli played for fc dinamo batumi and the olympique lyonnais club whose homeground is the parc olympique lyonnais stadium . <end>\n",
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC7HAFp3ck1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformer_discriminator import *\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idicZbPhdV9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "9f076365-e08d-4f4b-86ea-77b5b27f434f"
      },
      "source": [
        "\n",
        "def TransformerDiscriminator2(vocab_size, maxlen = 500,\n",
        "                             embed_dim = 32,  # Embedding size for each token\n",
        "                             num_heads = 2 ,  # Number of attention heads\n",
        "                             ff_dim = 32):    # # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "    #vocab_size = tokenizer_txt.vocab_size+2  # Only consider the top 20k words\n",
        "    #maxlen = 500  # Only consider the first 200 words of each movie review\n",
        "\n",
        "\n",
        "    inputs_rdf = layers.Input(shape=(maxlen,), name='rdf')\n",
        "    inputs_txt = layers.Input(shape=(maxlen,), name='txt')\n",
        "    \n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    \n",
        "    x_rdf = embedding_layer(inputs_rdf)\n",
        "    x_txt = embedding_layer(inputs_txt)\n",
        "\n",
        "    \n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "    \n",
        "    x_rdf = transformer_block(x_rdf)\n",
        "    x_rdf = layers.GlobalAveragePooling1D()(x_rdf)\n",
        "    x_rdf = layers.Dropout(0.1)(x_rdf)\n",
        "\n",
        "    x_txt = transformer_block(x_txt)\n",
        "    x_txt = layers.GlobalAveragePooling1D()(x_txt)\n",
        "    x_txt = layers.Dropout(0.1)(x_txt)\n",
        "\n",
        "    x = layers.concatenate([x_rdf, x_txt])\n",
        "\n",
        "    x = layers.Dense(20, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\", name='real_prob')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs_rdf, inputs_txt], outputs=outputs)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "discriminator = TransformerDiscriminator2(tokenizer_txt.vocab_size+2, maxlen=135)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "rdf (InputLayer)                [(None, 135)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "txt (InputLayer)                [(None, 135)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_and_position_embedding_1  (None, 135, 32)      344320      rdf[0][0]                        \n",
            "                                                                 txt[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_1 (Transforme (None, 135, 32)      6464        token_and_position_embedding_1[0]\n",
            "                                                                 token_and_position_embedding_1[1]\n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 32)           0           transformer_block_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_3 (Glo (None, 32)           0           transformer_block_1[1][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32)           0           global_average_pooling1d_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32)           0           global_average_pooling1d_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 64)           0           dropout_7[0][0]                  \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 20)           1300        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 20)           0           dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "real_prob (Dense)               (None, 1)            21          dropout_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 352,105\n",
            "Trainable params: 352,105\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Isc-lMzDbSzZ"
      },
      "source": [
        "## Train and Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WzDIyt2UbSzb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0a06cc66-8906-40aa-fdb0-e1852b4acdc9"
      },
      "source": [
        "discriminator.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "history = discriminator.fit(\n",
        "    {\"rdf\": Xr_train,\n",
        "     \"txt\": Xt_train},\n",
        "    {\"real_prob\": y_train},\n",
        "    epochs=2,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    validation_data=(\n",
        "                      {\"rdf\": Xr_test,\n",
        "                      \"txt\": Xt_test},\n",
        "                     {\"real_prob\": y_test}\n",
        "                    )\n",
        "                )"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1933/1933 [==============================] - 163s 85ms/step - loss: 0.1888 - accuracy: 0.9345 - val_loss: 0.1814 - val_accuracy: 0.9412\n",
            "Epoch 2/2\n",
            "1933/1933 [==============================] - 178s 92ms/step - loss: 0.1474 - accuracy: 0.9516 - val_loss: 0.1696 - val_accuracy: 0.9460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se800dK_0NAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator.save_weights('./discriminator_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTr9Wzmwf5vg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}